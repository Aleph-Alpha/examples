{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation, Prompt, TextControl\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.llms import AlephAlpha\n",
    "from langchain.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the API to create an embedding of a text\n",
    "Use the \"Semantic Search\" API to create an embedding of a text. The API returns a JSON object with the embedding of the text.\n",
    "You can read more about the semantic search API here: https://docs.aleph-alpha.com/docs/tasks/semantic_embed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use the API to find out how similar these three texts are\n",
    "Use scipys cosine_similarity 'spatial.distance.cosine' to find out how similar these three texts are. The function returns a number between 0 and 1, where 0 means the texts are completely different and 1 means the texts are identical.\n",
    "Remember that cosine_similarity returns a distance, not a similarity. So you need to subtract the distance from 1 to get the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The sun is shining\", \n",
    "         \"It's pretty sunny today\", \n",
    "         \"Her smile shines brightly down upon the south african people\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between The sun is and It's prett 0.822170248199528\n",
      "Similarity between text The sun is and Her smile :  0.3074451358909682\n",
      "Similarity between text It's prett and Her smile :  0.1735522296901626\n"
     ]
    }
   ],
   "source": [
    "# TODO: create an embedding request (Symmetric), get the response, and extract the embedding\n",
    "text_embeddings = []\n",
    "for text in texts:\n",
    "    embedding_request = SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Symmetric, compress_to_size=128)\n",
    "    embedding_response = client.semantic_embed(embedding_request, model=\"luminous-base\")\n",
    "    embedding = embedding_response.embedding\n",
    "    text_embeddings.append(embedding)\n",
    "\n",
    "# TODO: Calculate the cosine similarity between the embeddings\n",
    "similarity_1_2 = 1 - spatial.distance.cosine(text_embeddings[0], text_embeddings[1])\n",
    "similarity_1_3 = 1 - spatial.distance.cosine(text_embeddings[0], text_embeddings[2])\n",
    "similarity_2_3 = 1 - spatial.distance.cosine(text_embeddings[1], text_embeddings[2])\n",
    "\n",
    "print(f\"Similarity between {texts[0][:10]} and {texts[1][:10]}\", similarity_1_2)\n",
    "print(f\"Similarity between text {texts[0][:10]} and {texts[2][:10]}: \", similarity_1_3)\n",
    "print(f\"Similarity between text {texts[1][:10]} and {texts[2][:10]}: \", similarity_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between The sun is and It's prett 0.8222216781668\n",
      "Similarity between The sun is and Her smile  0.30750528534059873\n",
      "Similarity between It's prett and Her smile  0.17384478378490975\n"
     ]
    }
   ],
   "source": [
    "# semantic similarity with langchain    \n",
    "embeddings = AlephAlphaSymmetricSemanticEmbedding()\n",
    "\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        similarity = 1 - spatial.distance.cosine(text_embeddings[i], text_embeddings[j])\n",
    "        print(f\"Similarity between {texts[i][:10]} and {texts[j][:10]}\", similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use the API on an asymmetric embedding case to find the answer to the question\n",
    "Asymmetric embeddings are useful when you want to find the answer to a question. For example, if you want to find the answer to the question \"What is the capital of France?\", you can use the API to create an embedding of the question and an embedding of the answer. Then you can use the cosine_similarity function to find out how similar the question and the answer are. The answer is the one with the highest similarity.\n",
    "\n",
    "We will try this on parts of the manual.\n",
    "\n",
    "You can find the documentation on the asymmetric embedding here: https://docs.aleph-alpha.com/docs/tasks/semantic_embed/#code-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in the data.md file\n",
    "with open(\"data.md\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Split the data into a list of texts\n",
    "texts = data.split(\"#####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the text files\n",
    "question = \"What countries have social elements in their guidelines?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create embeddings (Document) for the contexts and the question (Query)\n",
    "embedded_contexts = []\n",
    "for context in texts:\n",
    "    # TODO: create an embedding request (Document), \n",
    "    # get the response, \n",
    "    # and extract the embedding\n",
    "    embedding_request = SemanticEmbeddingRequest(\n",
    "        prompt=Prompt.from_text(context), \n",
    "        representation=SemanticRepresentation.Document, \n",
    "        compress_to_size=128)\n",
    "    embedding_response = client.semantic_embed(embedding_request, model=\"luminous-base\")\n",
    "    embedding = embedding_response.embedding\n",
    "    embedded_contexts.append(embedding)\n",
    "\n",
    "# TODO\n",
    "# create an embedding request (Query), \n",
    "# get the response, \n",
    "# and extract the embedding\n",
    "embedded_question = client.semantic_embed(\n",
    "    SemanticEmbeddingRequest(\n",
    "        prompt=Prompt.from_text(question), \n",
    "        representation=SemanticRepresentation.Query, \n",
    "        compress_to_size=128), \n",
    "    model=\"luminous-base\").embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create embeddings (Document) for the contexts and the question (Query) with langchain\n",
    "\n",
    "# Load the embedding model\n",
    "embeddings = AlephAlphaAsymmetricSemanticEmbedding()\n",
    "\n",
    "# TODO: create embeddings (Document) for the contexts and the question (Query)\n",
    "embedded_contexts = embeddings.embed_documents(texts)\n",
    "embedded_question = embeddings.embed_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities:  [0.48128268240327854, 0.4867165345669583, 0.41384206688658354, 0.46313856409885634, 0.5188248431308402, 0.425083909884707, 0.4523307108843164, 0.37041205042762737, 0.4814904306643404, 0.4575652064311403, 0.4894331790145512, 0.41141384387662094]\n",
      "\n",
      "\n",
      "Selected Context: \n",
      " POLICY INTEGRATION\n",
      "\n",
      "POLICY INTEGRATION – **15**\n",
      "\n",
      "Sustainable development has been defined a variety of ways, but in practice it has come to mean development that achieves a balance among economic, environmental and social objectives for both present and future generations. The integration of the three dimensions of sustainable development is one of the most difficult balances to achieve in formulating a national strategy. In practice, most national strategies have a greater focus on environmental issues with some attempts to incorporate economic aspects. The social pillar has been the most neglected. As a result, few national strategies develop abilities for considering and making trade-offs among the three areas in overall policy-making.\n",
      "\n",
      "Most countries emphasize environmental components in their sustainable development strategies. For example, the main focus of the Danish strategy is on integrating environmental considerations into a number of specific sectors. The Australian Strategy for Ecologically Sustainable Development focuses on environmental concerns such as coastal zone management. Japan’s strategy is basically an environmental plan focused on materials recycling, global warming and biodiversity, while Korea’s strategy is based on the National Environmental Vision for the New Millennium. Canada has developed departmental strategies according to “A Guide to Green Government” to pursue the environmental health of the country.\n",
      "\n",
      "The social element is the dimension the least integrated in national strategies. Whereas these often include social goals, sometimes with indicators, social objectives are generally listed alongside other objectives and rarely melded into a comprehensive strategy. There are also large differences across countries in how they interpret this notion – from a focus on the health consequences of environmental policies, to concerns about ethnic minorities and gender balance, to broader considerations about the quality of life, sustainable consumption and social relations (poverty, crime, employment, education).\n",
      "\n",
      "GOOD PRACTICES IN THE NATIONAL SUSTAINABLE DEVELOPMENT STRATEGIES OF OECD COUNTRIES – © OECD 2006\n",
      "\n",
      "---\n",
      "\n",
      "A few countries *(e.g.,* Belgium, Germany, New Zealand, Sweden) have succeeded in integrating social elements in their strategies. For example, in 2005, Germany outlined its progress on the Road Map for Sustainability, which addresses employment, ageing, pension and family issues as well as corporate social responsibility.\n",
      "\n",
      "Some countries have extended social considerations from the domestic to the international sphere. Norway counts among its social sustainability goals the need to increase development assistance and imports from developing countries. Portugal’s national strategy includes guidelines for reinforcing development co-operation, especially with Portuguese speaking countries, and moving towards the target of devoting 0.7% of GDP to official development aid.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate the cosine similarity between the embeddings\n",
    "similarities = []\n",
    "for embedded_context in embedded_contexts:\n",
    "    # TODO: Calculate the cosine similarity between the embeddings\n",
    "    similarity = 1 - spatial.distance.cosine(embedded_context, embedded_question)\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "print(\"Similarities: \", similarities)\n",
    "print(\"\\n\\nSelected Context: \\n\" + texts[np.argmax(similarities)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe-hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fb2148b4dec95289a29917982720107fca12734259108fc6abb6a274b2eb7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
