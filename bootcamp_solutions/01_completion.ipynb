{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.llms import AlephAlpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# for using the Aleph Alpha API\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "# for using LangChain\n",
    "aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), model=\"luminous-extended\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the completion function of the API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the full-fledged AA API for a simple completion\n",
    "\n",
    "# Define the prompt\n",
    "prompt = Prompt.from_text(\"\"\"### Instructions: Write a poem about Hedgehogs.\n",
    "                          \n",
    "                          ### Response:\"\"\")\n",
    "\n",
    "# Create a completion request with parameters\n",
    "request = CompletionRequest(prompt=prompt, \n",
    "                            maximum_tokens=100, \n",
    "                            temperature=0.0, \n",
    "                            stop_sequences=[])\n",
    "\n",
    "# Send the request to the API\n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "\n",
    "# Get the completion\n",
    "completion = response.completions[0].completion\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LangChain for a simple completion\n",
    "\n",
    "# define the prompt\n",
    "prompt = \"An apple a day\"\n",
    "\n",
    "# define the parameters\n",
    "params = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"model\": \"luminous-extended\",\n",
    "    \"maximum_tokens\": 100\n",
    "}\n",
    "\n",
    "# define the model\n",
    "aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), **params)\n",
    "\n",
    "# get the completion\n",
    "response = aleph_alpha(prompt=prompt, stop=[\"\\n\"])\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Write a few shot prompt that generates keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's import some data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in the data.md file\n",
    "with open(\"new_data.md\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Split the data into a list of texts\n",
    "texts = data.split(\"#\")\n",
    "\n",
    "print(f\"data: {data[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's create Keywords for the first 10 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 texts\n",
    "texts_for_keywords = texts\n",
    "\n",
    "# TODO write a function that takes a text and returns a list of keywords\n",
    "def get_keywords(text):\n",
    "    # TODO Write a prompt that generates keywords for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    \n",
    "    # 1. define the prompt\n",
    "    prompt = Prompt.from_text(f\"\"\"Identify matching keywords for each text.\n",
    "###\n",
    "Text: The \"Whiskey War\" is an ongoing conflict between Denmark and Canada over ownership of Hans Island. The dispute began in 1973, when Denmark and Canada reached an agreement on Greenland's borders. However, no settlement regarding Hans Island could be reached by the time the treaty was signed. Since then both countries have used peaceful means - such as planting their national flag or burying liquor - to draw attention to the disagreement.\n",
    "Keywords: \n",
    "1. Conflict, \n",
    "2. Whiskey War, Denmark, Canada, Treaty, Flag, Liquor\n",
    "###\n",
    "Text: NASA launched the Discovery program to explore the solar system. It comprises a series of expeditions that have continued from the program's launch in the 1990s to the present day. In the course of the 16 expeditions launched so far, the Moon, Mars, Mercury and Venus, among others, have been explored. Unlike other space programs, the Discovery program places particular emphasis on cost efficiency, true to the motto: \"faster, better, cheaper\".\n",
    "Keywords: Space program, NASA, Expedition, Cost efficiency, Moon, Mars, Mercury, Venus\n",
    "###\n",
    "Text: {text}\n",
    "Keywords:\"\"\")   \n",
    "    \n",
    "    # 2. define the completion request \n",
    "    request = CompletionRequest(prompt=prompt, \n",
    "                                    maximum_tokens=32, \n",
    "                                    temperature=0, \n",
    "                                    stop_sequences=[\"\\n\"],\n",
    "                                    frequency_penalty=0.1, penalty_bias=\"\")\n",
    "    \n",
    "    # 3. send the request to the API\n",
    "    response = client.complete(request=request, model=\"luminous-supreme\")\n",
    "    \n",
    "    # 4. get the completion\n",
    "    completion = response.completions[0].completion\n",
    "    return completion\n",
    "\n",
    "def get_keywords_langchain(text):\n",
    "    # TODO Write a prompt that generates keywords for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    \n",
    "    # 1. define the prompt\n",
    "    prompt = f\"\"\"### Instruction: Extract keywords from the text. Enumerate the keywords. ### Input: Text:{text}\\n### Response: 1.\"\"\"\n",
    "    \n",
    "    # 2. define the parameters\n",
    "    params = {\n",
    "        \"temperature\": 0.5,\n",
    "        \"model\": \"luminous-base-control\",\n",
    "        \"maximum_tokens\": 100\n",
    "    }\n",
    "\n",
    "    # 3. define the model\n",
    "    aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), **params)\n",
    "    \n",
    "    # 4. get the completion\n",
    "    response = aleph_alpha(prompt=prompt, stop=[\"\\n\"])\n",
    "    return response\n",
    "    \n",
    "\n",
    "for text in texts:\n",
    "    keywords = get_keywords(text)\n",
    "    print(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Langchain to run a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a chain from langchain to generate a text\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"### Instructions: Solve the task based on the text below\".\n",
    "\n",
    "### Input:\n",
    "{context}\n",
    "\n",
    "### Task: {question}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "llm = AlephAlpha(model=\"luminous-base-control\", maximum_tokens=32, stop_sequences=[\"###:\"], aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What is the speed of digital acceleration?\"\n",
    "\n",
    "answer = llm_chain.run(question = question, context = texts[2])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write two prompts one, that generates a question about a text and one that then answers the questions\n",
    "\n",
    "def generate_questions_and_answers(text):\n",
    "    # TODO Write a prompt that generates questions and answers for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "        \n",
    "    # 1. TODO define the prompt\n",
    "    questions_prompt = Prompt.from_text(f\"\"\"### Instructions: Generate one specific question about the text below.\n",
    "                                        \n",
    "                                        ### Input: {text}\n",
    "                                        \n",
    "                                        ### Response:\"\"\")\n",
    "    \n",
    "    # 2. TODO define the completion request for the questions\n",
    "    request = CompletionRequest(prompt=questions_prompt, temperature=0.0, stop_sequences=[\"\\n\"])\n",
    "    response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "    question = response.completions[0].completion\n",
    "    \n",
    "    # 3. get the completion for answers\n",
    "    answers_prompt = Prompt.from_text(f\"\"\"### Instructions: Generate an answer to the question below based on the text.\n",
    "                                        \n",
    "                                        ### Input: {text}\n",
    "                                        \n",
    "                                        ### Question: {question}\n",
    "                                        \n",
    "                                        ### Response:\"\"\")   \n",
    "    request = CompletionRequest(prompt=answers_prompt, temperature=0.0, maximum_tokens=128)\n",
    "    response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "        \n",
    "    # 4. get the completion\n",
    "    completion = response.completions[0].completion\n",
    "    \n",
    "    return question, completion\n",
    "\n",
    "text = texts[1]\n",
    "\n",
    "question, completion = generate_questions_and_answers(text)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: \\n{completion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write a function where the user chats with an AI, store the conversation in a list as a memory\n",
    "history = []\n",
    "def chat_with_ai(message):\n",
    "    # TODO Write a prompt that generates questions and answers for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    history.append(f\"User: {message}\")\n",
    "    breaker = \"\\n\"\n",
    "    # 1. TODO define the prompt with the message and the history\n",
    "    prompt = Prompt.from_text(f\"\"\"### Instructions: You are a friendly and helpful AI. You are chatting with a human user. Try your best to answer their questions.\n",
    "\n",
    "### History: \n",
    "{breaker.join(history)}\n",
    "\n",
    "### AI:\"\"\")\n",
    "    \n",
    "    # 2. TODO define the completion request for the answer\n",
    "    request = CompletionRequest(prompt=prompt, temperature=0.5, stop_sequences=[\"\\n\"])\n",
    "    response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "    answer = response.completions[0].completion\n",
    "    \n",
    "    \n",
    "    \n",
    "    history.append(f\"AI: {answer}\")\n",
    "    \n",
    "    return \"\\n\".join(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_with_ai(\"What are attractions in the city?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
