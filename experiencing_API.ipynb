{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Aleph Alpha Technology\n",
    "Hi, great to see you working with Aleph Alpha Technology. This is a short tutorial to get you started with the API.\n",
    "\n",
    "This notebook will contain information on the following topics:\n",
    "1. Use our LLMs to generate text and solve tasks\n",
    "2. Using embeddings to find similar relevant information\n",
    "3. Use semantic embeddings and completion to answer questions\n",
    "4. Chaining multiple requests to solve complex tasks\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a colab notebook:\n",
    "!pip install aleph-alpha-client scipy qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These are just some imports to start working with our API\n",
    "If you are interested, here is what the individual imports do:\n",
    "\n",
    "| Import | Description |\n",
    "| --- | --- |\n",
    "| ``Client`` | This is the main class that you will use to authenticate with the API. |\n",
    "| ``Prompt`` | We use this class to format information correctly for our models |\n",
    "| ``CompletionRequest`` | CompletionRequests are used to reuqest our models to generate text, e.g. for solving tasks |\n",
    "| ``SemanticEmbeddingRequest`` | SemanticEmbeddingRequests are used to request our models to generate embeddings for text, e.g. for searching for information or for classification |\n",
    "| ``ExplanationRequest`` | ExplanationRequests are used to request our models to generate explanations for text, e.g. for explaining a where an answer comes from |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation, ExplanationRequest\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with the API by using the client class\n",
    "client = Client(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Using LLMs to generate text and solve tasks\n",
    "In this section, we will use our LLMs to generate text and solve tasks.\n",
    "\n",
    "We will use the same LLM for both tasks. This is because our LLMs are trained to solve many different tasks. This means that we can use the same LLM for many different tasks.\n",
    "\n",
    "We will use the completion endpoint to generate text and solve tasks. You can find more information about this endpoint in the [Completion Documentation](https://docs.aleph-alpha.com/docs/tasks/complete/).\n",
    "\n",
    "With completions we prompt the model to generate text. Depending on the prompt, the model will generate different text. This is a very powerful universal tool to generate text and solve tasks.\n",
    "\n",
    "However, to get the best results, we need to formulate our prompts correctly. We need to keep in mind the structure that the model expects and also how to word our requests so that the model understands what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generating text\n",
    "First, let's just start with generating text. While our API offers different models, we will start with our ``Control-models``. These models are specifically optimized to solve tasks that you give them.\n",
    "\n",
    "We will stick to the structure that these models expect. This is a good starting point to get familiar with the API.\n",
    "\n",
    "```markdown\n",
    "### Instruction:\n",
    "INPUT YOUR INSTRUCTION HERE\n",
    "\n",
    "### Input:\n",
    "YOUR INPUT\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "Try to vary the input and see how the model responds. You can also try to change the instruction and see how the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a prompt, so that the model knows what to do\n",
    "prompt_text = \"\"\"### Instruction: \n",
    "Complete the sentence below with a continuation that makes sense.\n",
    "\n",
    "### Input:\n",
    "An apple a day\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# Create the completion request\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(prompt_text), \n",
    "    maximum_tokens=20, # Parameter to control the maximum length of the completion\n",
    "    temperature=0.0, # Parameter to control the randomness of the completion\n",
    "    stop_sequences=[\"\\n\"]) # Parameter to control the stopping criteria of the completion\n",
    "\n",
    "# Send the prompt to the API\n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "\n",
    "response_text = response.completions[0].completion\n",
    "\n",
    "# Print the response\n",
    "print(f\"The model returned: `{response_text}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Solving specific tasks\n",
    "Now that we have seen how to generate text, let's try to solve a specific task. We will use the same model as before, but we will give it a different instruction.\n",
    "\n",
    "This time, we want to create a product text for a new product. We will give the model a short description of the product and ask it to generate a product text.\n",
    "\n",
    "We will be using both the ``Control-models`` as well as the ``foundation-models``. The ``foundation-models`` are trained on a large amount of data and are able to generate text that is more fluent and coherent. However, they are not as good at solving specific tasks as the ``Control-models``.\n",
    "\n",
    "While control models work with a specific structure, the foundation models are more flexible. This means that we can use them to generate text in a more natural way. However, they require a ``few-shot`` prompt. This means that we need to give them a few examples of what we want them to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a control model prompt for you to try out\n",
    "control_prompt_text = \"\"\"### Instruction:\n",
    "Generate a product description for the following product.\n",
    "Only use information from the product description.\n",
    "\n",
    "### Input:\n",
    "Name: Multifunctional Yoga Mat\n",
    "Color: Blue\n",
    "Material: Rubber\n",
    "Size: 180 x 60 x 0.5 cm\n",
    "Uses: Yoga, Pilates, Fitness, Gymnastics, Camping, Picnic, Sleep, Play, etc.\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# Let's send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(control_prompt_text),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.0,\n",
    "    stop_sequences=[])\n",
    "\n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "\n",
    "response_text = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned: `{response_text}`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we would write the prompt as a few-shot learning prompt\n",
    "few_shot_prompt_text = \"\"\"Task: Generate a product description for the following product.\n",
    "Only use information from the product description.\n",
    "###\n",
    "Product:\n",
    "- Name: Ergonomic Office Chair\n",
    "- Color: Black\n",
    "- Material: Plastic, Metal, Fabric\n",
    "- Functions: Height adjustable, 360 degree swivel, seat tilt, back tilt\n",
    "- Uses: Office, Home, Gaming, etc.\n",
    "Description: This ergonomic office chair is made of high-quality materials, such as plastic, metal, and fabric and is very comfortable to sit on. \n",
    "It is height adjustable, can swivel 360 degrees, and has a seat and back tilt. \n",
    "It is suitable for use in the office, at home, or for gaming.\n",
    "###\n",
    "Product:\n",
    "- Name: Multifunctional Yoga Mat\n",
    "- Color: Blue\n",
    "- Material: Rubber\n",
    "- Size: 180 x 60 x 0.5 cm\n",
    "- Uses: Yoga, Pilates, Fitness, Gymnastics, Camping, Picnic, Sleep, Play, etc.\n",
    "Description:\"\"\"\n",
    "\n",
    "# Let's send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(few_shot_prompt_text),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.5, # We can use a higher temperature to make the model more creative\n",
    "    stop_sequences=[\"###\"] # with the foundation models we need to specify the stop sequence\n",
    "    )\n",
    "\n",
    "response = client.complete(request=request, model=\"luminous-extended\")\n",
    "\n",
    "\n",
    "response_text = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned: `{response_text}`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Experiment with completions LLMs yourself\n",
    "\n",
    "Now you can go ahead and experiment with completions yourself. \n",
    "\n",
    "Try to solve different tasks with the LLMs. \n",
    "\n",
    "Experiment with ``Control-models`` and ``foundation-models``. \n",
    "See how they differ in their responses and how they solve tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Change the prompt to be solve a different task\n",
    "control_prompt_text = \"\"\"Try to write your own prompt here.\"\"\"\n",
    "\n",
    "# Send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(    \n",
    "    prompt=Prompt.from_text(control_prompt_text),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.0,\n",
    "    stop_sequences=[])\n",
    "\n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "response_text = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned: `{response_text}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Using Embeddings to search for information\n",
    "In many cases, the relevant information to solve a task may not be available or known to the model.\n",
    "\n",
    "With Semantic Search, we can use the embeddings to search for relevant information in a corpus of documents. The idea is that LLMs are able to understand the meaning of a question and the meaning of a document, and thus, can find the most relevant document to answer the question.\n",
    "\n",
    "We do this by first encoding the question and the documents into embeddings. Then, we compute the similarity between the question embedding and the document embeddings. Finally, we return the document with the highest similarity score.\n",
    "\n",
    "You can find more information about this technique in the [Semantic Embedding Documentation](https://docs.aleph-alpha.com/docs/tasks/semantic_embed/).\n",
    "\n",
    "Let's see how this works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating embeddings for text.\n",
    "In Order to find the correct documents, we need to turn our text into numbers.\n",
    "We do that with semnatic embeddings. These are vectors that represent the meaning of the data.\n",
    "\n",
    "Let's use Aleph Alpha technology to create embeddings for our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two texts and a question to be embedded and searched for\n",
    "text_1 = \"With our semantic_embed-endpoint you can create semantic embeddings for your text. This functionality can be used in a myriad of ways. For more information please check out our blog-post on Luminous-Explore, introducing the model behind the semantic_embed-endpoint. In order to effectively search through your own documents, it is important to ensure that they can be easily compared to each other. Our asymmetric embeddings are designed to help find the pieces of your documents that are most relevant to a query shorter than the documents in the database. Here we will use short queries and longer splits of law texts.\"\n",
    "text_2 = \"You can interact with a Luminous model by sending it a text. We call this a prompt. It will then produce text that continues your input and return it to you. This is what we call a completion. Generally speaking, our models attempt to find the best continuation for a given input. Practically, this means that the model first recognizes the style of the prompt and then attempts to continue it accordingly.\"\n",
    "\n",
    "question = \"How can I search through my documents with embeddings?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the API to embed the text\n",
    "\n",
    "# We embed the texts as Documents, as the contain a lot of information\n",
    "request_1 = SemanticEmbeddingRequest(prompt=Prompt.from_text(text_1), representation=SemanticRepresentation.Document)\n",
    "request_2 = SemanticEmbeddingRequest(prompt=Prompt.from_text(text_2), representation=SemanticRepresentation.Document)\n",
    "\n",
    "# We embed the question as a Query, as it is a short text\n",
    "request_question = SemanticEmbeddingRequest(prompt=Prompt.from_text(question), representation=SemanticRepresentation.Query)\n",
    "\n",
    "# We send the requests to the API\n",
    "embedding_1 = client.semantic_embed(request_1, model=\"luminous-base\").embedding\n",
    "embedding_2 = client.semantic_embed(request_2, model=\"luminous-base\").embedding\n",
    "embedding_question = client.semantic_embed(request_question, model=\"luminous-base\").embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Calculating the similarity between embeddings\n",
    "Now that we have embeddings for our question and our documents, we can calculate the similarity between them.\n",
    "For that we use the cosine similarity. This is a measure of how similar two vectors are. The higher the value, the more similar the vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We calculate the cosine similarity between the question and the texts\n",
    "similarity_1 = 1 - spatial.distance.cosine(embedding_1, embedding_question)\n",
    "similarity_2 = 1 - spatial.distance.cosine(embedding_2, embedding_question)\n",
    "\n",
    "# We print the results\n",
    "print(\"The similarity between the question and text 1 is: \" + str(similarity_1))\n",
    "print(\"The similarity between the question and text 2 is: \" + str(similarity_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the document with the highest similarity score is the one that we are looking for.\n",
    "This semantic search is a very powerful tool to find relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experiment with embeddings yourself\n",
    "Now you can go ahead and experiment with embeddings yourself. \n",
    "\n",
    "When do they work well? \n",
    "\n",
    "When do they not work well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Change the text to be embedded and searched for\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# TODO Change the question to be embedded and searched for\n",
    "test_question = \"What does the fox do?\"\n",
    "\n",
    "# run the code to embed the text and question and calculate the similarity\n",
    "request_test_text = SemanticEmbeddingRequest(prompt=Prompt.from_text(test_text), representation=SemanticRepresentation.Document)\n",
    "request_test_question = SemanticEmbeddingRequest(prompt=Prompt.from_text(test_question), representation=SemanticRepresentation.Query)\n",
    "embedding_test_text = client.semantic_embed(request_test_text, model=\"luminous-base\").embedding\n",
    "embedding_test_question = client.semantic_embed(request_test_question, model=\"luminous-base\").embedding\n",
    "similarity_test = 1 - spatial.distance.cosine(embedding_test_text, embedding_test_question)\n",
    "print(\"The similarity between the question and text 1 is: \" + str(similarity_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Using Semantic Embeddings and Completions together to answer questions\n",
    "In this section, we will use the search and completions endpoints together to answer questions.\n",
    "\n",
    "With `semantic search`, we can find relevant information. With `completions`, we can generate text and solve tasks.\n",
    "\n",
    "Our application logic is as follows:\n",
    "1. We use `semantic search` to make information searchable.\n",
    "2. We select the most similar document as background information.\n",
    "3. We use `completions` to generate the answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reuse the texts from the previous task\n",
    "text_1 = \"With our semantic_embed-endpoint you can create semantic embeddings for your text. This functionality can be used in a myriad of ways. For more information please check out our blog-post on Luminous-Explore, introducing the model behind the semantic_embed-endpoint. In order to effectively search through your own documents, it is important to ensure that they can be easily compared to each other. Our asymmetric embeddings are designed to help find the pieces of your documents that are most relevant to a query shorter than the documents in the database. Here we will use short queries and longer splits of law texts.\"\n",
    "text_2 = \"You can interact with a Luminous model by sending it a text. We call this a prompt. It will then produce text that continues your input and return it to you. This is what we call a completion. Generally speaking, our models attempt to find the best continuation for a given input. Practically, this means that the model first recognizes the style of the prompt and then attempts to continue it accordingly.\"\n",
    "\n",
    "question = \"How can I search through my documents with embeddings?\"\n",
    "\n",
    "\n",
    "# Creatting the embedding for the texts and the question\n",
    "request_1 = SemanticEmbeddingRequest(prompt=Prompt.from_text(text_1), representation=SemanticRepresentation.Document)\n",
    "request_2 = SemanticEmbeddingRequest(prompt=Prompt.from_text(text_2), representation=SemanticRepresentation.Document)\n",
    "request_question = SemanticEmbeddingRequest(prompt=Prompt.from_text(question), representation=SemanticRepresentation.Query)\n",
    "embedding_1 = client.semantic_embed(request_1, model=\"luminous-base\").embedding\n",
    "embedding_2 = client.semantic_embed(request_2, model=\"luminous-base\").embedding\n",
    "embedding_question = client.semantic_embed(request_question, model=\"luminous-base\").embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Using a vectordatabase to store embeddings\n",
    "Instead of doin g everything ourselves, we can use a Vectordatabase to store the embeddings for us. This makes it easier to search for information.\n",
    "\n",
    "We will be using qdrant as our vectordatabase. \n",
    "Qdrant is an open-source vectordatabase that is easy to use and fast.\n",
    "You can find more information about qdrant [here](https://qdrant.tech/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we spin up the Qdrant server\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
    "\n",
    "q_client = QdrantClient(path=\"db\")\n",
    "\n",
    "q_client.recreate_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to store the documents in the vectordatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create embeddings for each of the texts and store them in a list\n",
    "texts = [text_1, text_2]\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    # embed the texts\n",
    "    embeddings.append(client.semantic_embed(SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Document, compress_to_size=128), model=\"luminous-base\").embedding)\n",
    "    \n",
    "    \n",
    "# now we can upsert the data into Qdrant\n",
    "ids = list(range(len(texts)))\n",
    "payloads = [{\"text\": text} for text in texts]\n",
    "\n",
    "q_client.upsert(\n",
    "     collection_name=\"test_collection\",\n",
    "     points=Batch(\n",
    "     ids=ids,\n",
    "     payloads=payloads,\n",
    "     vectors=embeddings\n",
    "     )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2 Using semantic search to find relevant information\n",
    "\n",
    "Now that we have stored our documents in the vectordatabase, we can use semantic search to find relevant information.\n",
    "\n",
    "For that we just have to send the embeddings of our question to the vectordatabase and it will return the most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding the question\n",
    "\n",
    "# We embed the question as a Query, as it is a short text\n",
    "request_question = SemanticEmbeddingRequest(\n",
    "    prompt=Prompt.from_text(question), \n",
    "    representation=SemanticRepresentation.Query, \n",
    "    compress_to_size=128)\n",
    "\n",
    "embedding_question = client.semantic_embed(request_question, model=\"luminous-base\").embedding\n",
    "\n",
    "search_result = q_client.search(\n",
    "        collection_name=\"test_collection\",\n",
    "        query_vector=embedding_question,\n",
    "        limit=5, # Parameter to control the number of results\n",
    "    )\n",
    "\n",
    "for result in search_result:\n",
    "    print(f\"Score: {result.score}, Text: {result.payload['text']}\")\n",
    "    \n",
    "    \n",
    "# Let's select the first result to answer the question\n",
    "background_text = search_result[0].payload[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3 Using completions to generate the answer\n",
    "\n",
    "Now that we have found the most relevant document, we can use completions to generate the answer.\n",
    "\n",
    "We will use the same model as before. However, this time we will give it a different instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = f\"\"\"### Instruction:\n",
    "{question}\n",
    "\n",
    "### Input:\n",
    "{background_text}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# Let's send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(qa_prompt),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.0,\n",
    "    stop_sequences=[\"\\n\"])\n",
    "\n",
    "response = client.complete(request=request, model=\"luminous-supreme-control\")\n",
    "\n",
    "response_text = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned: `{response_text}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Chaining multiple requests to solve complex tasks\n",
    "Sometimes, we need to solve complex tasks. For that, we can chain multiple requests together.\n",
    "\n",
    "Similar to Humans, LLMs produce more robust results if they are able to solve a task in multiple steps. This is because they can focus on one task at a time and do not have to solve everything at once (end-to-end).\n",
    "\n",
    "While solving tasks end-to-end may be very convenient, it is not always the best solution. This is because the model may not be able to focus on the most important parts of the task. This can lead to worse results.\n",
    "\n",
    "It is also much more difficult to debug and understand what the model is doing. This is because the model is solving the task in one step and we cannot see what it is doing.\n",
    "\n",
    "In this example, we will be summarizing a support request. However, instead of just doing this end-to-end, we will first find the relevant information and then summarize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Extracting the relevant Information from the support request\n",
    "\n",
    "First, let's extract some general information from the support request.\n",
    "This will help us generate a more structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the support request\n",
    "support_request = \"\"\"Dear support Team,\n",
    "I was trying to use one of these cool language models from Aleph Alpha, but I am having trouble with the API.\n",
    "I am getting the following error message:\n",
    "```\n",
    "AttributeError: 'CompletionResponse' object has no attribute 'text'\n",
    "```\n",
    "Could you please help me with this?\n",
    "\n",
    "Best,\n",
    "Markus Schmitz\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a control prompt for extracting the relevant information.\n",
    "Please keep in mind that this is just an example.\n",
    "\n",
    "You might need to adjust the prompt to your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's use luminous to extract the most important information from the support request\n",
    "\n",
    "data_extraction_prompt = f\"\"\"### Instruction:\n",
    "Extract the most important information from the support request as a JSON object.\n",
    "These are:\n",
    "- The \"name\" of the person\n",
    "- The \"task\" that the person is trying to accomplish\n",
    "- Error message\n",
    "\n",
    "### Input:\n",
    "{support_request}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "# Let's send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(data_extraction_prompt),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.0,\n",
    "    stop_sequences=[])\n",
    "    \n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "extracted_data = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned: \\n{extracted_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = f\"\"\"### Instruction:\n",
    "Write a summary of the support request in one sentence.\n",
    "\n",
    "### Input:\n",
    "{support_request}\n",
    "\n",
    "### Response:\n",
    "Summary:\"\"\" # here we write summary to indicate that the model should write a summary\n",
    "\n",
    "# Let's send the prompt to the API and see what the model returns\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(summary_prompt),\n",
    "    maximum_tokens=100,\n",
    "    temperature=0.0,\n",
    "    stop_sequences=[])\n",
    "\n",
    "response = client.complete(request=request, model=\"luminous-extended-control\")\n",
    "summary = response.completions[0].completion\n",
    "\n",
    "print(f\"The model returned the summary: \\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Putting it all together\n",
    "\n",
    "Now that we have extracted the relevant information, and created a summary, we can put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ticket using the extracted data and the summary\n",
    "\n",
    "ticket = {\n",
    "    \"metadata\": extracted_data,\n",
    "    \"summary\": summary,\n",
    "    \"text\": support_request\n",
    "}\n",
    "# saving the ticket as a json file\n",
    "with open(\"ticket.json\", \"w\") as f:\n",
    "    json.dump(ticket, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AtMan: Understanding the model's decisions\n",
    "This section will show you how to use AtMan to understand the model's decisions.\n",
    "\n",
    "With our `explain`-endpoint you can get an explanation of the model's output. In more detail, we return how much the log-probabilites of the already generated completion would change if we supress indivdual parts (based on the granularity you chose) of a prompt. Please refer to this part of our documentation if you would like to know more about our explainability method in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_text = \"\"\"Answer the question based on the context.\n",
    "\n",
    "Context: According to tradition, on April 21, 753 BC, Romulus and his twin brother Remus founded Rome in the place where they had been suckled as orphans by a she-wolf.\n",
    "\n",
    "Q: In which month was Rome founded?\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"prompt\": Prompt.from_text(prompt_text),\n",
    "    \"maximum_tokens\": 1,\n",
    "}\n",
    "request = CompletionRequest(**params)\n",
    "response = client.complete(request=request, model=\"luminous-supreme\")\n",
    "completion = response.completions[0].completion\n",
    "\n",
    "exp_req = ExplanationRequest(Prompt.from_text(prompt_text), completion, prompt_granularity=\"paragraph\")\n",
    "response_explain = client.explain(exp_req, model=\"luminous-supreme\")\n",
    "\n",
    "explanations = response_explain[1][0].items[0][0]\n",
    "\n",
    "for item in explanations:\n",
    "    start = item.start\n",
    "    end = item.start + item.length\n",
    "    print(f\"\"\"EXPLAINED TEXT: {prompt_text[start:end]}\n",
    "Score: {np.round(item.score, decimals=3)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the example. The explanation helps us locate the relevant information that Luminous used.\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial, we have seen how to use our API to generate text, search for information, and solve tasks.\n",
    "\n",
    "We have also seen how to chain multiple requests together to solve complex tasks.\n",
    "\n",
    "We hope that this tutorial was helpful to you. If you have any questions, please do not hesitate ask us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "templates",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
