{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a colab notebook:\n",
    "#!git clone https://github.com/Aleph-Alpha/examples.git\n",
    "#!pip install -r examples/requirements.txt\n",
    "#!cp examples/bootcamp/data.md data.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Completions\n",
    "In this notebook we will learn how to use the API to create completions.\n",
    "\n",
    "Follow the instructions below to get started!\n",
    "\n",
    "Whenever there is a TODO, you will need to fill in the code to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "\n",
    "from langchain.llms import AlephAlpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Define some helper functions\n",
    "Here we initialize the client and submit our Token in order to authenticate when using the API.\n",
    "\n",
    "You can create a new Token in the playground. Visit https://app.aleph-alpha.com/profile to create a new API token.\n",
    "\n",
    "When working locally, you can add it to the .env file in the root directory of the project.\n",
    "\n",
    "Otherwise you can set it as the token as a string. E.g.: \n",
    "    \n",
    "```python\n",
    "client = Client(token=\"YOUR_TOKEN\")\n",
    "``````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# for using the Aleph Alpha API\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "# for using LangChain\n",
    "aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), model=\"luminous-extended\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the completion function of the API\n",
    "When calling the API, we want to define the parameters of how Luminous should solve the task.\n",
    "\n",
    "The API has a completion function that takes a task and a set of parameters as input and returns a completion as output.\n",
    "\n",
    "The completion function is called `complete` and is part of the `luminous` API module.\n",
    "\n",
    "\n",
    "In this case we already provided the prompt for you. Your task is to define the parameters for the completion function.\n",
    "\n",
    "You can find the documentation for the completion function [here](https://docs.aleph-alpha.com/docs/tasks/complete/).\n",
    "\n",
    "Feel free to play around with the parameters and see how they affect the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the full-fledged AA API for a simple completion\n",
    "\n",
    "# Define the prompt\n",
    "prompt = Prompt.from_text(\"\"\"### Instructions: Complete the following sentence with a few words.\n",
    "\n",
    "### Input: An apple a day\n",
    "\n",
    "### Response:\"\"\")\n",
    "\n",
    "# Create a completion request with parameters\n",
    "request = None # TODO: create a completion request with the prompt and parameters https://docs.aleph-alpha.com/docs/tasks/complete/ \n",
    "\n",
    "# Send the request to the API\n",
    "response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "\n",
    "# Get the completion\n",
    "completion = response.completions[0].completion\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "Great Job! You've completed the first lesson!\n",
    "Now let's try to do the same thing using `Langchain`.\n",
    "Langchain is a library that provides a common interface to iinteract with different models and services.\n",
    "It is designed to be easy to use and to be easily extensible.\n",
    "\n",
    "You can find the documentation [here](https://langchain.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LangChain for a simple completion\n",
    "\n",
    "# define the prompt\n",
    "prompt = \"An apple a day\"\n",
    "\n",
    "# define the parameters\n",
    "params = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"model\": \"luminous-extended\",\n",
    "    \"maximum_tokens\": 100\n",
    "}\n",
    "\n",
    "# define the model\n",
    "aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), **params)\n",
    "\n",
    "# get the completion\n",
    "response = aleph_alpha(prompt=prompt, stop=[\"\\n\"])\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Write a few shot prompt that generates keywords\n",
    "\n",
    "Now that we know how to use the API, let's write a few shot prompt that generates keywords. \n",
    "\n",
    "First we will read in some text from a file. Then we will use the API to generate keywords from that text. Finally, we will print out the keywords."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's import some data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in the data.md file\n",
    "with open(\"data.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Split the data into a list of texts\n",
    "texts = data.split(\"#\")\n",
    "\n",
    "# remove the first element of the list\n",
    "texts = texts[1:]\n",
    "\n",
    "print(f\"data: {data[:100]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "#### Let's create Keywords for the first rows of the dataset\n",
    "\n",
    "We've already provided the function itself. Now you need to write a good prompt in order to get the desired result.\n",
    "\n",
    "Try to either use `Few-shot` or `Zero-shot` learning.\n",
    "\n",
    "You can find some tips on prompting the models here: \n",
    "\n",
    "https://docs.aleph-alpha.com/docs/introduction/zero-shot_prompting/\n",
    "\n",
    "https://docs.aleph-alpha.com/docs/introduction/few_shot_prompting/\n",
    "\n",
    "You can use either the Aelph Alpha Client or Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first 10 texts\n",
    "texts_for_keywords = texts\n",
    "\n",
    "# Let's write a function that takes a text and returns a list of keywords\n",
    "def get_keywords(text):\n",
    "    # TODO Write a prompt that generates keywords for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    \n",
    "    # 1. TODO define the prompt\n",
    "    prompt = Prompt.from_text(f\"\"\"A good Keyword prompt for {text}\"\"\")   \n",
    "    \n",
    "    # 2. define the completion request \n",
    "    request = CompletionRequest(prompt=prompt, \n",
    "                                    maximum_tokens=32, \n",
    "                                    temperature=0, \n",
    "                                    stop_sequences=[\"\\n\"])\n",
    "    \n",
    "    # 3. send the request to the API\n",
    "    response = client.complete(request=request, model=\"luminous-base\")\n",
    "    \n",
    "    # 4. get the completion\n",
    "    completion = response.completions[0].completion\n",
    "    return completion\n",
    "\n",
    "# Alternatively you can write the function with langchain\n",
    "def get_keywords_langchain(text):\n",
    "    # TODO Write a prompt that generates keywords for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    \n",
    "    # 1. TODO define the prompt\n",
    "    prompt = f\"\"\"Prompt\"\"\"\n",
    "    \n",
    "    # 2. define the parameters\n",
    "    params = {\n",
    "        \"temperature\": 0.5,\n",
    "        \"model\": \"luminous-base-control\",\n",
    "        \"maximum_tokens\": 100\n",
    "    }\n",
    "\n",
    "    # 3. define the model\n",
    "    aleph_alpha = AlephAlpha(aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"), **params)\n",
    "    \n",
    "    # 4. get the completion\n",
    "    response = aleph_alpha(prompt=prompt, stop=[\"\\n\"])\n",
    "    return response\n",
    "    \n",
    "\n",
    "for text in texts:\n",
    "    keywords = get_keywords(text)\n",
    "    print(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### Step 3: Let's use Langchain to run a chain\n",
    "\n",
    "Let's see how we can use Langchain to answer a question about a context.\n",
    "\n",
    "In this example we use a LLMChain and provide a context and a question. The model will then try to answer the question based on the context.\n",
    "\n",
    "Its your job to write a prompt that works well with the model.\n",
    "\n",
    "Use `context` and `question` to create a prompt that will answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a chain from langchain to generate a text\n",
    "\n",
    "\n",
    "template = \"\"\"{context}, {question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"context\"])\n",
    "\n",
    "llm = AlephAlpha(model=\"luminous-base-control\", maximum_tokens=32, stop_sequences=[\"###:\"], aleph_alpha_api_key=os.getenv(\"AA_TOKEN\"))\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"When was the declaration signed?\"\n",
    "\n",
    "answer = llm_chain.run(question = question, context = texts[1])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Step 4: generate Questions and Answers\n",
    "\n",
    "In this section we will use luminous sequentially to generate questions and answers for the given text. We will use the `generate_questions_and_answers` function to generate questions and answers for the given text. \n",
    "\n",
    "Your job is to write bothe the question generation prompt as well as the answer generation prompt.\n",
    "\n",
    "Feel free to use a previous prompt if you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write two prompts one, that generates a question about a text and one that then answers the questions\n",
    "\n",
    "def generate_questions_and_answers(text):\n",
    "    # TODO Write a prompt that generates questions and answers for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "        \n",
    "    # 1. TODO define the prompt\n",
    "    questions_prompt = Prompt.from_text(f\"\"\"prompt\"\"\")\n",
    "    \n",
    "    # 2. TODO define the completion request for the questions\n",
    "    request = CompletionRequest(prompt=questions_prompt, temperature=0.0, stop_sequences=[\"\\n\"])\n",
    "    response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "    question = response.completions[0].completion\n",
    "    \n",
    "    # 3. TODO get the completion for answers\n",
    "    answers_prompt = Prompt.from_text(f\"\"\"prompt\"\"\")   \n",
    "    request = CompletionRequest(prompt=answers_prompt, temperature=0.0, maximum_tokens=128)\n",
    "    response = client.complete(request=request, model=\"luminous-base-control\")\n",
    "        \n",
    "    # 4. get the completion\n",
    "    completion = response.completions[0].completion\n",
    "    \n",
    "    return question, completion\n",
    "\n",
    "text = texts[0]\n",
    "\n",
    "question, completion = generate_questions_and_answers(text)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: \\n{completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "### Step 5: Chatting with Luminous\n",
    "\n",
    "Finally, let's write a function so that we can chat with Luminous.\n",
    "We provided a lot of the code below.\n",
    "\n",
    "Again you must find a good prompt to make Luminous say something interesting.\n",
    "\n",
    "**Hint**: We are using the ``history`` parameter to hold the state with the previous messages. You can use this to make Luminous remember things you said in the past.\n",
    "\n",
    "Please keep in mind that `Luminous` is not optimzed for chat and will not be able to hold a conversation for long. It is just a fun example of how you can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a function where the user chats with an AI, store the conversation in a list as a memory\n",
    "history = []\n",
    "def chat_with_ai(message):\n",
    "    # TODO Write a prompt that generates questions and answers for any text\n",
    "    # Tipp: Use few-shot learning\n",
    "    history.append(f\"User: {message}\")\n",
    "    breaker = \"\\n\"\n",
    "    # 1. TODO try varyinf the prompt\n",
    "    prompt = Prompt.from_text(f\"\"\"### Instructions: ...\n",
    "\n",
    "### History: \n",
    "{breaker.join(history)}\n",
    "\n",
    "### AI:\"\"\")\n",
    "    \n",
    "    # 2. TODO define the completion request for the answer\n",
    "    request = CompletionRequest(prompt=prompt, temperature=0.5,)\n",
    "    response = client.complete(request=request, model=\"luminous-extended-control\")\n",
    "    answer = response.completions[0].completion\n",
    "    \n",
    "    \n",
    "    \n",
    "    history.append(f\"AI: {answer}\")\n",
    "    \n",
    "    return \"\\n\".join(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_with_ai(\"Maybe you can help me out. What are the most importnat things to know about boats?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
