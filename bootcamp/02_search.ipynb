{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation, Prompt, TextControl\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.llms import AlephAlpha\n",
    "from langchain.embeddings import AlephAlphaSymmetricSemanticEmbedding, AlephAlphaAsymmetricSemanticEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Use the API to create an embedding of a text\n",
    "Use the \"Semantic Search\" API to create an embedding of a text. The API returns a JSON object with the embedding of the text.\n",
    "You can read more about the semantic search API here: https://docs.aleph-alpha.com/docs/tasks/semantic_embed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use the API to find out how similar these three texts are\n",
    "Use scipys cosine_similarity 'spatial.distance.cosine' to find out how similar these three texts are. The function returns a number between 0 and 1, where 0 means the texts are completely different and 1 means the texts are identical.\n",
    "Remember that cosine_similarity returns a distance, not a similarity. So you need to subtract the distance from 1 to get the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The sun is shining\", \n",
    "         \"It's pretty sunny today\", \n",
    "         \"Her smile shines brightly down upon the south african people\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create an embedding request (Symmetric), get the response, and extract the embedding\n",
    "text_embeddings = []\n",
    "for text in texts:\n",
    "    embedding_request = SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Symmetric, compress_to_size=128)\n",
    "    embedding_response = client.semantic_embed(embedding_request, model=\"luminous-base\")\n",
    "    embedding = embedding_response.embedding\n",
    "    text_embeddings.append(embedding)\n",
    "\n",
    "# TODO: Calculate the cosine similarity between the embeddings\n",
    "similarity_1_2 = 1 - spatial.distance.cosine(text_embeddings[0], text_embeddings[1])\n",
    "similarity_1_3 = 1 - spatial.distance.cosine(text_embeddings[0], text_embeddings[2])\n",
    "similarity_2_3 = 1 - spatial.distance.cosine(text_embeddings[1], text_embeddings[2])\n",
    "\n",
    "print(f\"Similarity between {texts[0][:10]} and {texts[1][:10]}\", similarity_1_2)\n",
    "print(f\"Similarity between text {texts[0][:10]} and {texts[2][:10]}: \", similarity_1_3)\n",
    "print(f\"Similarity between text {texts[1][:10]} and {texts[2][:10]}: \", similarity_2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic similarity with langchain    \n",
    "embeddings = AlephAlphaSymmetricSemanticEmbedding()\n",
    "\n",
    "text_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        similarity = 1 - spatial.distance.cosine(text_embeddings[i], text_embeddings[j])\n",
    "        print(f\"Similarity between {texts[i][:10]} and {texts[j][:10]}\", similarity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Use the API on an asymmetric embedding case to find the answer to the question\n",
    "Asymmetric embeddings are useful when you want to find the answer to a question. For example, if you want to find the answer to the question \"What is the capital of France?\", you can use the API to create an embedding of the question and an embedding of the answer. Then you can use the cosine_similarity function to find out how similar the question and the answer are. The answer is the one with the highest similarity.\n",
    "\n",
    "We will try this on parts of the manual.\n",
    "\n",
    "You can find the documentation on the asymmetric embedding here: https://docs.aleph-alpha.com/docs/tasks/semantic_embed/#code-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the text files\n",
    "contexts = [\"Germany : Germany (, ), officially the Federal Republic of Germany, is a country in Central Europe. It is the second most populous country in Europe after Russia, and the most populous member state of the European Union. Germany is situated between the Baltic and North seas to the north, and the Alps to the south; it covers an area of , with a population of over 83 million within its 16 constituent states. Germany borders Denmark to the north, Poland and the Czech Republic to the east, Austria and Switzerland to the south, and France, Luxembourg, Belgium, and the Netherlands to the west. The nation's capital and largest city is Berlin, and its financial centre is Frankfurt; the largest urban area is the Ruhr.\",\n",
    "            \"Bristol : Bristol () is a city, ceremonial county and unitary authority in England. Situated on the River Avon, it is bordered by the ceremonial counties of Gloucestershire, to the north; and Somerset, to the south. Bristol is the most populous city in South West England.\\nThe wider Bristol Built-up Area is the eleventh most populous urban area in the United Kingdom.\",\n",
    "            \"Heidelberg : Heidelberg () is a university town in the German state of Baden-W\\u00fcrttemberg, situated on the river Neckar in south-west Germany. In the 2016 census, its population was 159,914, of which roughly a quarter consisted of students.\",\n",
    "            \"France : France (), officially the French Republic (), is a transcontinental country spanning Western Europe and overseas regions and territories in the Americas and the Atlantic, Pacific and Indian Oceans. Its metropolitan area extends from the Rhine to the Atlantic Ocean and from the Mediterranean Sea to the English Channel and the North Sea; overseas territories include French Guiana in South America, Saint Pierre and Miquelon in the North Atlantic, the French West Indies, and many islands in Oceania and the Indian Ocean. Due to its several coastal territories, France has the largest exclusive economic zone in the world. France borders Belgium, Luxembourg, Germany, Switzerland, Monaco, Italy, Andorra and Spain in Europe, as well as the Netherlands, Suriname and Brazil in the Americas. Its eighteen integral regions (five of which are overseas) span a combined area of  and over 67 million people (). France is a unitary semi-presidential republic with its capital in Paris, the country's largest city and main cultural and commercial centre; other major urban areas include Marseille, Lyon, Toulouse, Lille, Bordeaux, and Nice.\"\n",
    "            ]\n",
    "        \n",
    "question = \"What city is located at the river Neckar?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create embeddings (Document) for the contexts and the question (Query)\n",
    "embedded_contexts = []\n",
    "for context in contexts:\n",
    "    # TODO: create an embedding request (Document), \n",
    "    # get the response, \n",
    "    # and extract the embedding\n",
    "    embedding_request = SemanticEmbeddingRequest(\n",
    "        prompt=Prompt.from_text(context), \n",
    "        representation=SemanticRepresentation.Document, \n",
    "        compress_to_size=128)\n",
    "    embedding_response = client.semantic_embed(embedding_request, model=\"luminous-base\")\n",
    "    embedding = embedding_response.embedding\n",
    "    embedded_contexts.append(embedding)\n",
    "\n",
    "# TODO\n",
    "# create an embedding request (Query), \n",
    "# get the response, \n",
    "# and extract the embedding\n",
    "embedded_question = client.semantic_embed(\n",
    "    SemanticEmbeddingRequest(\n",
    "        prompt=Prompt.from_text(question), \n",
    "        representation=SemanticRepresentation.Query, \n",
    "        compress_to_size=128), \n",
    "    model=\"luminous-base\").embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create embeddings (Document) for the contexts and the question (Query) with langchain\n",
    "\n",
    "# Load the embedding model\n",
    "embeddings = AlephAlphaAsymmetricSemanticEmbedding()\n",
    "\n",
    "# TODO: create embeddings (Document) for the contexts and the question (Query)\n",
    "embedded_contexts = embeddings.embed_documents(contexts)\n",
    "embedded_question = embeddings.embed_query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the cosine similarity between the embeddings\n",
    "similarities = []\n",
    "for embedded_context in embedded_contexts:\n",
    "    # TODO: Calculate the cosine similarity between the embeddings\n",
    "    similarity = 1 - spatial.distance.cosine(embedded_context, embedded_question)\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "print(\"Similarities: \", similarities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Let's try to split the third text into sections and find the answer to the question\n",
    "The third text is a bit longer than the other two texts. Let's try to split the text into sections and find the answer to the question.\n",
    "The splitting is already done for you. You just need to create an embedding of each section and find the section with the highest similarity to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find the most similar context\n",
    "similarities = []\n",
    "for embeddeding in embedded_contexts:\n",
    "    # TODO calculate the cosine similarity between the embeddings https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
    "    similarities.append(None)\n",
    "\n",
    "# print out the text of the most similar context\n",
    "print(\"Most similar context: \", contexts[np.argmax(similarities)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe-hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fb2148b4dec95289a29917982720107fca12734259108fc6abb6a274b2eb7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
