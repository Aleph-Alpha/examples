{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Let's do a really simple QA prompt\n",
    "In this step we will ask luminous a question from the manual. Let's see how it answers.\n",
    "Write a completionrequest and make luminous answer it. Don't forget the stop sequences.\n",
    "You can find the documentation for the completionrequest here: https://docs.aleph-alpha.com/docs/tasks/complete/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Q: How is Stop Category 0 defined?\\nA:\"\n",
    "\n",
    "# TODO: Write the completion request\n",
    "request = None\n",
    "\n",
    "# TODO: Call the client to run the completionrequest\n",
    "response = None\n",
    "print(response.completions[0].completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "prompt = \"Q: How is Stop Category 0 defined?\\nA:\"\n",
    "\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(prompt),\n",
    "    maximum_tokens=64,\n",
    "    stop_sequences=[\"Q:\", \"Q\"],\n",
    ")\n",
    "\n",
    "response = client.complete(request = request, model = \"luminous-extended\")\n",
    "print(response.completions[0].completion)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Let's do a QA prompt with a context\n",
    "Obviously, we can't expect luminous to answer the question if we don't give it any context. Let's see how it answers when we give it some context.\n",
    "Write the new prompt and make luminous answer it. Don't forget the stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"The robot has protective and emergency stop functions (stop category 0 or 1, in accordance with IEC 60204-1).\n",
    "\n",
    "| Stop category 0 | As defined in IEC 60204-1, stopping by immediate removal of power to the machine actuators. |  \n",
    "|---|---|  \n",
    "| Stop category 1 | As defined in IEC 60204-1, a controlled stop with power avail- able to the machine actuators to achieve the stop and then re- moval of power when the stop is achieved. |  \"\"\"\n",
    "\n",
    "# TODO write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"Prompt that includes the {context}\"\n",
    "\n",
    "request = None\n",
    "\n",
    "response = None\n",
    "print(response.completions[0].completion)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "context = \"\"\"he robot has protective and emergency stop functions (stop category 0 or 1, in accordance with IEC 60204-1).\n",
    "\n",
    "| Stop category 0 | As defined in IEC 60204-1, stopping by immediate removal of power to the machine actuators. |  \n",
    "|---|---|  \n",
    "| Stop category 1 | As defined in IEC 60204-1, a controlled stop with power avail- able to the machine actuators to achieve the stop and then re- moval of power when the stop is achieved. |  \"\"\"\n",
    "\n",
    "# TODO write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"This system answers questions about the robot based on the context. \\n\\nContext: {context}\\nQ: How is Stop Category 0 defined?\\nA:\"\n",
    "\n",
    "request = CompletionRequest(\n",
    "    prompt=Prompt.from_text(prompt),\n",
    "    maximum_tokens=64,\n",
    "    stop_sequences=[\"Q:\", \"Q\"],\n",
    ")\n",
    "\n",
    "response = client.complete(request = request, model = \"luminous-extended\")\n",
    "print(response.completions[0].completion)\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Let's use our search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question\n",
    "This step is a bit long, but it is a good exercise to see how we can use the search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Pinocchio was not a boy, but a wooden puppet. He was made by a carpenter named Geppetto. He was a very naughty puppet. He was always getting into trouble. He was always lying.\",\n",
    "    \"Most commonly associated with the polar regions, permafrost is soil and rocky material that stays frozen continuously for at least two years. Normally it lies beneath an active layer that melts and freezes depending on the season. Less well known is that permafrost can also be found on steep mountain walls.\",\n",
    "    \"Toheroa are a clam that grow as large as a human hand and burrow in intertidal sands on just a handful of epic surf-swept beaches – mainly on the west coast of New Zealand's North Island, but also in isolated colonies at places like Oreti, a beach at the nation's southern tip.\",\n",
    "    \"On the neighbourhood's southern edge, cutting through Queens like a backbone, is Roosevelt Avenue. Here, conversations don't stop when the 7 train rattles overhead, they just get louder. Phone repair shops run by Tibetans with makeshift shrines displayed between plastic iPhone covers abut Latin American bakeries churning out pillowy almojábanas (Colombian cheese bread) and crispy empanadas.\"\n",
    "]\n",
    "        \n",
    "question = \"What city is located at the river Neckar?\"\n",
    "\n",
    "#TODO: embed the contexts\n",
    "embedded_texts = []\n",
    "for text in texts:\n",
    "    pass\n",
    "\n",
    "# TODO: embed the question\n",
    "embedded_question = None\n",
    "\n",
    "# TODO: calculate the cosine similarity between the question and the contexts\n",
    "similarities = []\n",
    "for embedded_text in embedded_texts:\n",
    "    pass\n",
    "\n",
    "# Get the text in the most similar context\n",
    "selected_context = texts[np.argmax(similarities)]\n",
    "\n",
    "# TODO: write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"Another Prompt\"\n",
    "\n",
    "# TODO: write the CompletionRequest\n",
    "request = None\n",
    "\n",
    "response = None\n",
    "\n",
    "print(response.completions[0].completion)\n",
    "print(selected_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "splitted_context = open(\"sample_file1.md\").read().split(\"######\")\n",
    "# remove small splits\n",
    "splits = []\n",
    "for split in splitted_context:\n",
    "    if len(split) > 30:\n",
    "        splits.append(split)\n",
    "        \n",
    "question = \"How is Stop Category 0 defined?\"\n",
    "\n",
    "#TODO: embed the contexts\n",
    "embedded_splits = []\n",
    "for split in splits:\n",
    "    embedded_splits.append(client.semantic_embed(SemanticEmbeddingRequest(prompt=Prompt.from_text(split), representation=SemanticRepresentation.Document, compress_to_size=128), model=\"luminous-base\").embedding)\n",
    "\n",
    "# TODO: embed the question\n",
    "embedded_question = client.semantic_embed(SemanticEmbeddingRequest(prompt=Prompt.from_text(question), representation=SemanticRepresentation.Document, compress_to_size=128), model=\"luminous-base\").embedding\n",
    "\n",
    "# TODO: calculate the cosine similarity between the question and the contexts\n",
    "similarities = []\n",
    "for embedded_split in embedded_splits:\n",
    "    similarities.append(1 - spatial.distance.cosine(embedded_question, embedded_split))\n",
    "\n",
    "# Get the text in the most similar context\n",
    "selected_context = splits[np.argmax(similarities)]\n",
    "\n",
    "# TODO: write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"This system answers questions about the robot based on the context. \\n\\nContext: {context}\\nQ: How is Stop Category 0 defined?\\nA:\"\n",
    "\n",
    "# TODO: write the CompletionRequest\n",
    "request = CompletionRequest(\n",
    "    prompt = Prompt.from_text(prompt),\n",
    "    maximum_tokens = 32,\n",
    "    stop_sequences=[\"\\n\\n\", \"Q:\", \"Q\"],\n",
    ")\n",
    "\n",
    "response = client.complete(request = request, model = \"luminous-extended\")\n",
    "\n",
    "print(response.completions[0].completion)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe-hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fb2148b4dec95289a29917982720107fca12734259108fc6abb6a274b2eb7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
