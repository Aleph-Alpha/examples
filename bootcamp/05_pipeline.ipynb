{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Let's do a really simple QA prompt\n",
    "In this step we will ask luminous a question from the manual. Let's see how it answers.\n",
    "Write a completionrequest and make luminous answer it. Don't forget the stop sequences.\n",
    "You can find the documentation for the completionrequest here: https://docs.aleph-alpha.com/docs/tasks/complete/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Q: Is the world flat?\\nA:\"\n",
    "\n",
    "# TODO: Write the completion request\n",
    "request = None\n",
    "\n",
    "# TODO: Call the client to run the completionrequest\n",
    "response = None\n",
    "print(response.completions[0].completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Let's do a QA prompt with a context\n",
    "Obviously, we can't expect luminous to answer the question if we don't give it any context. Let's see how it answers when we give it some context.\n",
    "Write the new prompt and make luminous answer it. Don't forget the stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"The robot has protective and emergency stop functions (stop category 0 or 1, in accordance with IEC 60204-1).\n",
    "\n",
    "| Stop category 0 | As defined in IEC 60204-1, stopping by immediate removal of power to the machine actuators. |  \n",
    "|---|---|  \n",
    "| Stop category 1 | As defined in IEC 60204-1, a controlled stop with power avail- able to the machine actuators to achieve the stop and then re- moval of power when the stop is achieved. |  \"\"\"\n",
    "\n",
    "# TODO write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"\"\"prompt\"\"\"\n",
    "\n",
    "# TODO: Write the completion request\n",
    "request = CompletionRequest()\n",
    "\n",
    "# Call the client to run the completionrequest\n",
    "response = client.complete(request, model=\"luminous-base-control-beta\")\n",
    "print(response.completions[0].completion)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Let's use our search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question\n",
    "This step is a bit long, but it is a good exercise to see how we can use the search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Pinocchio was not a boy, but a wooden puppet. He was made by a carpenter named Geppetto. He was a very naughty puppet. He was always getting into trouble. He was always lying.\",\n",
    "    \"Most commonly associated with the polar regions, permafrost is soil and rocky material that stays frozen continuously for at least two years. Normally it lies beneath an active layer that melts and freezes depending on the season. Less well known is that permafrost can also be found on steep mountain walls.\",\n",
    "    \"Toheroa are a clam that grow as large as a human hand and burrow in intertidal sands on just a handful of epic surf-swept beaches found mainly on the west coast of New Zealand's North Island, but also in isolated colonies at places like Oreti, a beach at the nation's southern tip.\",\n",
    "    \"On the neighbourhood's southern edge, cutting through Queens like a backbone, is Roosevelt Avenue. Here, conversations don't stop when the 7 train rattles overhead, they just get louder. Phone repair shops run by Tibetans with makeshift shrines displayed between plastic iPhone covers abut Latin American bakeries churning out pillowy almoj√°banas (Colombian cheese bread) and crispy empanadas.\"\n",
    "]\n",
    "        \n",
    "question = \"Who made Pinocchio?\"\n",
    "\n",
    "#TODO: embed the contexts\n",
    "embedded_texts = []\n",
    "for text in texts:\n",
    "    pass\n",
    "\n",
    "# TODO: embed the question\n",
    "embedded_question = None\n",
    "\n",
    "# TODO: calculate the cosine similarity between the question and the contexts\n",
    "similarities = []\n",
    "for embedded_text in embedded_texts:\n",
    "    pass\n",
    "# Get the text in the most similar context\n",
    "selected_context = texts[np.argmax(similarities)]\n",
    "\n",
    "# TODO: write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"\"\"prompt\"\"\"\n",
    "\n",
    "# TODO: write the CompletionRequest\n",
    "request = CompletionRequest()\n",
    "\n",
    "# TODO: Call the client to run the completionrequest\n",
    "response = client.complete()\n",
    "\n",
    "print(response.completions[0].completion)\n",
    "print(selected_context)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this with a more complex setup and a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we spin up the Qdrant server\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
    "\n",
    "q_client = QdrantClient(path=\"db\")\n",
    "\n",
    "q_client.recreate_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we load our data from data.md\n",
    "\n",
    "with open(\"data.md\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Lets split the data by headings (###)\n",
    "data = data.split(\"###\")\n",
    "\n",
    "# remove the first element as it is empty\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create embeddings for each of the texts and store them in a list\n",
    "embeddings = []\n",
    "for text in data:\n",
    "    # TODO: embed the texts\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can upsert the data into Qdrant\n",
    "ids = list(range(len(data)))\n",
    "payloads = [{\"text\": text} for text in data]\n",
    "\n",
    "q_client.upsert(\n",
    "     collection_name=\"test_collection\",\n",
    "     points=Batch(\n",
    "     ids=ids,\n",
    "     payloads=payloads,\n",
    "     vectors=embeddings\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO write a function that takes a question and returns an answer by searching in the Qdrant database\n",
    "def search_and_answer(question):\n",
    "    # TODO First we embed the question\n",
    "    embedded_question = client.semantic_embed().embedding\n",
    "    \n",
    "    # Then we search for the most similar text\n",
    "    search_result = q_client.search(\n",
    "        collection_name=\"test_collection\",\n",
    "        query_vector=embedded_question,\n",
    "        filter=None,\n",
    "        top=1\n",
    "    )\n",
    "    \n",
    "    # Then we get the text from the search result\n",
    "    text = search_result[0].payload[\"text\"]\n",
    "    \n",
    "    # TODO Finally we ask luminous to answer the question based on the text\n",
    "    prompt = f\"\"\"prompt\"\"\"\n",
    "    \n",
    "    # TODO write the CompletionRequest\n",
    "    request = CompletionRequest()\n",
    "    \n",
    "    # TODO get the response from luminous\n",
    "    response = client.complete()\n",
    "    \n",
    "    return response.completions[0].completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_answer(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_answer(\"What can I do with the community edition?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe-hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fb2148b4dec95289a29917982720107fca12734259108fc6abb6a274b2eb7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
