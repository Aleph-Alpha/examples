{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using a colab notebook:\n",
    "#!wget https://github.com/Aleph-Alpha/examples/blob/main/bootcamp/data.md\n",
    "#!wget https://github.com/Aleph-Alpha/examples/blob/main/requirements.txt\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aleph_alpha_client import Client, Prompt, CompletionRequest, CompletionResponse, SemanticEmbeddingRequest, SemanticEmbeddingResponse, SemanticRepresentation\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = Client(token=os.getenv(\"AA_TOKEN\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Let's do a really simple QA prompt\n",
    "In this step we will ask luminous a question from the manual. Let's see how it answers.\n",
    "Write a completionrequest and make luminous answer it. Don't forget the stop sequences.\n",
    "You can find the documentation for the completionrequest here: https://docs.aleph-alpha.com/docs/tasks/complete/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Q: What is stop category 0?\\nA:\"\n",
    "\n",
    "# TODO: Write the completion request\n",
    "request = None \n",
    "\n",
    "# TODO: Call the client to run the completionrequest\n",
    "response = client.complete(request, model=\"luminous-base-control\")\n",
    "print(response.completions[0].completion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Let's do a QA prompt with a context\n",
    "Obviously, we can't expect luminous to answer the question if we don't give it any context. Let's see how it answers when we give it some context.\n",
    "Write the new prompt and make luminous answer it. Don't forget the stop sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"The robot has protective and emergency stop functions (stop category 0 or 1, in accordance with IEC 60204-1).\n",
    "\n",
    "| Stop category 0 | As defined in IEC 60204-1, stopping by immediate removal of power to the machine actuators. |  \n",
    "|---|---|  \n",
    "| Stop category 1 | As defined in IEC 60204-1, a controlled stop with power avail- able to the machine actuators to achieve the stop and then re- moval of power when the stop is achieved. |  \"\"\"\n",
    "\n",
    "# TODO write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"\"\"Prompt\"\"\"\n",
    "\n",
    "request = CompletionRequest(prompt=Prompt.from_text(prompt), maximum_tokens=32)\n",
    "\n",
    "response = client.complete(request, model=\"luminous-base-control\")\n",
    "print(response.completions[0].completion)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Let's use our search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question\n",
    "This step is a bit long, but it is a good exercise to see how we can use the search we coded in the previous notebook to find the relevant context for the question and then use that context to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data in the data.md file\n",
    "with open(\"new_data.md\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "# Split the data into a list of texts\n",
    "texts = data.split(\"#\")\n",
    "texts = texts[1:]\n",
    "\n",
    "print(f\"data: {data[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are current macro trends in the market?\"\n",
    "\n",
    "#TODO: embed the contexts\n",
    "\n",
    "embedded_texts = []\n",
    "for text in texts:\n",
    "    pass # TODO create the embeddings and add them to the list\n",
    "\n",
    "# TODO: embed the question\n",
    "embedded_question = None\n",
    "\n",
    "# calculate the cosine similarity between the question and the contexts\n",
    "similarities = []\n",
    "for embedded_text in embedded_texts:\n",
    "    similarities.append(1 - spatial.distance.cosine(embedded_text, embedded_question))\n",
    "\n",
    "# Get the text in the most similar context\n",
    "selected_context = texts[np.argmax(similarities)]\n",
    "\n",
    "# TODO: write a prompt that makes luminous answer the question based on the context\n",
    "prompt = f\"\"\"Prompt\"\"\"\n",
    "\n",
    "# TODO: write the CompletionRequest\n",
    "request = None # TODO: create the request\n",
    "\n",
    "response = client.complete(request, model=\"luminous-base-control\")\n",
    "\n",
    "print(\"Model response: \", response.completions[0].completion)\n",
    "print(\"\\n\\n selected_context: \", selected_context)\n",
    "print(similarities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this with a more complex setup and a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we spin up the Qdrant server\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, Batch\n",
    "\n",
    "q_client = QdrantClient(path=\"db\")\n",
    "\n",
    "q_client.recreate_collection(\n",
    "    collection_name=\"test_collection\",\n",
    "    vectors_config=VectorParams(size=128, distance=Distance.COSINE),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create embeddings for each of the texts and store them in a list\n",
    "embeddings = []\n",
    "for text in texts:\n",
    "    # TODO: embed the texts\n",
    "    embeddings.append(client.semantic_embed(SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Document, compress_to_size=128), model=\"luminous-base\").embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can upsert the data into Qdrant\n",
    "ids = list(range(len(texts)))\n",
    "payloads = [{\"text\": text} for text in texts]\n",
    "\n",
    "q_client.upsert(\n",
    "     collection_name=\"test_collection\",\n",
    "     points=Batch(\n",
    "     ids=ids,\n",
    "     payloads=payloads,\n",
    "     vectors=embeddings\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write a function that takes a question and returns an answer by searching in the Qdrant database\n",
    "def search_and_answer(question):\n",
    "    # First we embed the question\n",
    "    embedded_question = client.semantic_embed(SemanticEmbeddingRequest(prompt=Prompt.from_text(question), representation=SemanticRepresentation.Query, compress_to_size=128), model=\"luminous-base\").embedding\n",
    "    \n",
    "    # Then we search for the most similar text\n",
    "    search_result = q_client.search(\n",
    "        collection_name=\"test_collection\",\n",
    "        query_vector=embedded_question,\n",
    "        filter=None,\n",
    "        top=1\n",
    "    )\n",
    "    \n",
    "    # return \"no answer found\" if no result has a score above 0.3\n",
    "    if search_result[0].score < 0.3:\n",
    "        return \"no answer found\"\n",
    "    \n",
    "    # Then we get the text from the search result\n",
    "    text = search_result[0].payload[\"text\"]\n",
    "    \n",
    "    # Finally we ask luminous to answer the question based on the text\n",
    "    prompt = f\"\"\"### Instructions: Answer the question briefly based on the provided Input.\n",
    "    \n",
    "    ### Input: {text}\n",
    "    \n",
    "    ### Question: {question}\n",
    "    \n",
    "    ### Response:\"\"\"\n",
    "    \n",
    "    # TODO write the CompletionRequest\n",
    "    request = CompletionRequest(prompt=Prompt.from_text(prompt), maximum_tokens=64, stop_sequences=[\"###\"])\n",
    "    \n",
    "    # TODO get the response from luminous\n",
    "    response = client.complete(request, model=\"luminous-base-control\")\n",
    "    \n",
    "    return response.completions[0].completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_answer(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_answer(\"What are current macro trends in the market?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpe-hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5fb2148b4dec95289a29917982720107fca12734259108fc6abb6a274b2eb7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
