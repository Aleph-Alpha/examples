{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aleph-Alpha/examples/blob/main/exercises/08_exercise_g.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise G: Create a Demo for Aleph Alpha Technology\n",
    "Finally, in this notebook we will try to create a demo application with a small interface\n",
    "\n",
    "This will help to assess how good a task can be completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aleph_alpha_client gradio\n",
    "\n",
    "from typing import Sequence\n",
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, SemanticEmbeddingRequest, SemanticRepresentation, Prompt, SummarizationRequest, CompletionRequest, EvaluationRequest, Document\n",
    "import gradio as gr\n",
    "\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the client and model\n",
    "search_model = AlephAlphaModel.from_model_name(\"luminous-base\", token=\"<Your API token>\")\n",
    "\n",
    "model = AlephAlphaModel.from_model_name(\"luminous-extended\", token=\"<Your API token>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a small data extraction demo\n",
    "We will create a small demo that extracts the most relevant information from emails. We will visualize the results in a small interface using gradio. \n",
    "\n",
    "The data we will extract is:\n",
    "- The sender\n",
    "- The receiver\n",
    "- The intent of the email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : write prompts that extract the sender and the receiver from the email.\n",
    "# Keep in mind that the prompt should be able to handle multiple emails and that there may be no information about the sender or the receiver.\n",
    "\n",
    "def extract_sender(text: str):\n",
    "    prompt = \"\"\"Write a prompt that extracts the sender from the email.\"\"\" #TODO\n",
    "    \n",
    "    request = CompletionRequest(prompt=Prompt.from_text(prompt), stop_sequences=[\"###\"], maximum_tokens=20)\n",
    "    completion = model.complete(request).completions[0].completion\n",
    "    sender = completion\n",
    "    return sender\n",
    "\n",
    "def extract_receiver(text: str):\n",
    "    prompt = \"\"\"Write a prompt that extracts the receiver from the email.\"\"\" #TODO\n",
    "    \n",
    "    request = CompletionRequest(prompt=Prompt.from_text(prompt), stop_sequences=[\"###\"], maximum_tokens=20)\n",
    "    completion = model.complete(request).completions[0].completion\n",
    "    receiver = completion\n",
    "    return receiver\n",
    "\n",
    "# TODO : write a prompt that extracts the intent from the email.\n",
    "def extract_intent(text: str):\n",
    "    prompt = \"\"\"Write a prompt that extracts the intent from the email.\"\"\" #TODO\n",
    "    \n",
    "    request = CompletionRequest(prompt=Prompt.from_text(prompt), stop_sequences=[\"###\"], maximum_tokens=20)\n",
    "    completion = model.complete(request).completions[0].completion\n",
    "    intent = completion\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper fuction that runs the above functions and returns the results\n",
    "def run(text: str):\n",
    "    sender = extract_sender(text)\n",
    "    receiver = extract_receiver(text)\n",
    "    intent = extract_intent(text)\n",
    "    return sender, receiver, intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "AI can be hard to understand. To make it easier for us to understand the results we will visualize them using gradio. Gradio is a library that allows you to create a small interface for your AI model. You can find more information about gradio [here](https://gradio.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Interface that accepts a text and returns the sender, receiver and intent\n",
    "interface = gr.Interface(\n",
    "    fn=run,\n",
    "    inputs=gr.components.Textbox(lines=10, placeholder=\"Enter your email here\"),\n",
    "    outputs=[\n",
    "        gr.components.Textbox(label=\"Sender\"),\n",
    "        gr.components.Textbox(label=\"Receiver\"),\n",
    "        gr.components.Textbox(label=\"Intent\"),\n",
    "    ],\n",
    "    title=\"Email Intent Extraction\",\n",
    "    description=\"This model extracts the sender, receiver and intent from an email.\"\n",
    ")\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 11:38:51) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bb351cbb231ebe1f2609a46f6d0b60d5d0bc334d8d2f0479e7f916a63419382"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
