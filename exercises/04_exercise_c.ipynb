{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Aleph-Alpha/examples/blob/main/exercises/04_exercise_c.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise C: Some Optimizations\n",
    "\n",
    "Use your prompting skills to do some optimization.\n",
    "\n",
    "How would you test if the prompt is working well?\n",
    "\n",
    "Try to use the client to automate as many steps as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aleph_alpha_client\n",
    "from aleph_alpha_client import Client, CompletionRequest, Prompt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the client and model\n",
    "client = Client(token=\"AA_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple functions for running the same prompt multiple times\n",
    "\n",
    "Here you can see a few simple functions that can be used to run a task for different inputs.\n",
    "Often it makes sense to define a specific few-shot prompt for a specific task, so that task can be repeated easily.\n",
    "This way, a new request for that task can be made and executed.\n",
    "The few-shot should contain a good structure and few-shot examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for running multiple completions\n",
    "def run_prompts_from_list(few_shot:str, inputs):\n",
    "    # iterate through all tasks\n",
    "    for input in inputs:\n",
    "        #create the prompt and request\n",
    "        prompt = Prompt.from_text(few_shot.format(input))\n",
    "        request = CompletionRequest(prompt=prompt, maximum_tokens=50, stop_sequences=[\"###\"])\n",
    "        # run the single task\n",
    "        result = client.complete(request, model=\"luminous-extended\")\n",
    "        #display the result\n",
    "        print(result.completions[0].completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: \n",
    "1. Copy your prompt from the last exercise and see if you can optimize it\n",
    "    - You can use the `%%time` magic to see how long it takes to run\n",
    "    - try experimenting with instructions and examples. Which works best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a few-shot prompt that works well on the task\n",
    "few_shot = \"\"\"This system generates titles for articles.\n",
    "###\n",
    "Text: Heidelberg is a city in Germany. It is beautiful and has a wonderful castle and many other sights worth visiting.\n",
    "Title: Heidelberg, the beautiful city\n",
    "###\n",
    "Text: This is an interpretation of Alice in Wonderland. Alice is a young girl who falls down a rabbit hole and finds herself in a strange world.\n",
    "Title: The tale of young Alice\n",
    "###\n",
    "Text: {}\n",
    "Title:\n",
    "\"\"\"\n",
    "\n",
    "# create a list of tasks to solve\n",
    "inputs = [\n",
    "    \"Dinosaurs roamed the earth millions of years ago. They were huge and scary. They were also very interesting.\",\n",
    "    \"The sun is a star. It is very hot and gives us light and warmth. It is also very big.\",\n",
    "    \"The works of Dionysius the Areopagite are a collection of writings from the 5th century. They are very important for the history of Christianity.\",\n",
    "]\n",
    "\n",
    "# run the prompts\n",
    "run_prompts_from_list(few_shot, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with supreme-control"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our steerable model Luminous-supreme-control has been optimized to work well with zero-shot instructions. This means that they do not necessarily need a set of examples like in few-shot learning. \n",
    "\n",
    "[Docs on supreme-control](https://docs.aleph-alpha.com/docs/introduction/prompting_and_completion/#zero-shot-learning-with-luminous-supreme-control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= \"\"\"This system answers questions based on its context. If the answer is not in the context, answer with \"No Answer\".\n",
    "Context:Born in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yesâ€“no questions can never be answered by computation and defined a Turing machine, and went on to prove that the halting problem for Turing machines is undecidable.\n",
    "Q: What was the profession of Alan Turing's father?\n",
    "A:\"\"\"\n",
    "\n",
    "request = CompletionRequest(prompt=Prompt.from_text(prompt), maximum_tokens=50, stop_sequences=[])\n",
    "\n",
    "response = client.complete(request, model=\"luminous-supreme-control\")\n",
    "print(response.completions[0].completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bb351cbb231ebe1f2609a46f6d0b60d5d0bc334d8d2f0479e7f916a63419382"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
