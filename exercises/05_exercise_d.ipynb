{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise D: A first look into search\n",
    "In this notebook we will take a look at luminous explore.\n",
    "\n",
    "Luminous-Explore is our model for semantic similarity.\n",
    "\n",
    "With explore, you can use the meaning of text to create awesome applications.\n",
    "\n",
    "Try it out below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some addition imports for search\n",
    "from typing import Sequence\n",
    "from aleph_alpha_client import ImagePrompt, AlephAlphaClient, AlephAlphaModel, SemanticEmbeddingRequest, SemanticRepresentation, Prompt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model for embedding\n",
    "\n",
    "Our Embedding model is called luminous-explore.\n",
    "\n",
    "However to access its functionalities, it is important to define luminous-base as the used model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the client and model\n",
    "model = AlephAlphaModel(\n",
    "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=os.getenv(\"API_TOKEN\")),\n",
    "    model_name = \"luminous-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple functions for embedding and searching\n",
    "Here we provide a simple function for embedding text and a simple function for calculating the similarity between two texts.\n",
    "\n",
    "Please take a moment to understand what each function does.\n",
    "\n",
    "The cosine similarity is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.\n",
    "\n",
    "You don't have to understand the details of the code, but you should understand the general idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for symmetric embedding\n",
    "def embed_symmetric(text: str):\n",
    "    # Create an embeddingrequest with the type set to symmetric\n",
    "    request = SemanticEmbeddingRequest(prompt=Prompt.from_text(text), representation=SemanticRepresentation.Symmetric)\n",
    "    # create the embedding\n",
    "    result = model.semantic_embed(request)\n",
    "    return result.embedding\n",
    "\n",
    "# function to calculate similarity\n",
    "def cosine_similarity(v1: Sequence[float], v2: Sequence[float]) -> float:\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: \n",
    "1. Play around with the semantic similarity of the word embeddings\n",
    "    - What difference does language make?\n",
    "    - What difference does the size of the text make?\n",
    "    - Can you find a semantic opposite of a text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9123379711230551\n"
     ]
    }
   ],
   "source": [
    "# define the texts\n",
    "text_a = \"The sun is shining\"\n",
    "text_b = \"Il sole splende\"\n",
    "\n",
    "# show the similarity\n",
    "print(cosine_similarity(embed_symmetric(text_a), embed_symmetric(text_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The embedding\n",
    "Let's also take a look at the embedding itself and how it looks.\n",
    "\n",
    "In the cell below we print the first 100 elements of the embedding.\n",
    "\n",
    "The embedding is 5120 elements long, so printing all of it would be quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2265625, 0.71875, -0.076660156, 0.79296875, -0.17480469, -1.9921875, 0.91015625, -1.1484375, 0.42578125, -1.1328125, 1.09375, 0.24804688, 0.85546875, -1.3359375, -0.60546875, -0.96484375, 0.07128906, -0.390625, -1.28125, -0.0009765625, 0.46679688, 1.46875, 0.29492188, 1.234375, 0.40625, -0.5078125, -2.078125, 2.703125, 0.17285156, -1.9609375, 0.6796875, 2.09375, -1.015625, -0.47851562, 1.109375, 0.076660156, -2.3125, -2.65625, -0.000831604, 0.9296875, 0.18945312, -1.109375, 0.44921875, -0.54296875, 1.359375, 0.7734375, 0.796875, 0.953125, -2.34375, -0.48632812, -0.42382812, -0.5625, 1.46875, 0.55078125, 0.18554688, 0.20214844, -0.040283203, -0.22558594, 0.4453125, 1.359375, 0.2734375, -2.578125, -0.25585938, -0.07519531, 0.15136719, -0.39453125, -1.1875, 0.87890625, 0.038330078, -1.5625, 0.048339844, 0.8203125, 1.2734375, 0.022094727, 1.1328125, -0.033935547, 1.6796875, -0.22070312, 0.06591797, -0.032226562, -0.67578125, -2.078125, -2.234375, -1.34375, -0.53515625, -1.4609375, -0.29882812, -1.28125, 0.111328125, -0.63671875, 1.2265625, 0.16210938, -0.19726562, 0.65625, -1.8359375, -0.053222656, -0.22460938, 0.48828125, -2.21875, -1.4921875]\n",
      "CPU times: user 8.73 ms, sys: 0 ns, total: 8.73 ms\n",
      "Wall time: 400 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(embed_symmetric(text_a)[:100])\n",
    "print(\"\\n\")\n",
    "print(embed_symmetric(text_b)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8bb351cbb231ebe1f2609a46f6d0b60d5d0bc334d8d2f0479e7f916a63419382"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
