{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: The Python Client\n",
    "\n",
    "In this notebook we will take a look at the basics of the Python client.\n",
    "\n",
    "We will use the client to query the API and make a simple completion call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages\n",
    "First we import the AlephAlphaClient with the classes for models and prompts.\n",
    "This is necessary to make the API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install aleph_alpha_client\n",
    "from aleph_alpha_client import AlephAlphaModel, AlephAlphaClient, CompletionRequest, Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the API and defining the model\n",
    "With the *AlephAlphaModel* class we define which model we want to use and provide our token for authentification.\n",
    "\n",
    "The token can be retrieved or just passed as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the client and model and provide token for authentification\n",
    "model = AlephAlphaModel.from_model_name(model_name=\"luminous-extended\", token=\"API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a prompt and task\n",
    "The prompt is the input to our models and can be defined with the *Prompt* class. You can read more about it [here](https://docs.aleph-alpha.com/docs/introduction/prompting_and_completion/).\n",
    "\n",
    "We also define a *CompletionRequest* class which we use to define the Model behaviour.\n",
    "\n",
    "With *model.complete* we send the prompt to the API and get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\"Q: What is AI? A:\")\n",
    "\n",
    "request = CompletionRequest(prompt=prompt, maximum_tokens=20, stop_sequences=[\"Q:\"])\n",
    "\n",
    "completion_response = model.complete(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the result\n",
    "\n",
    "A completion is the result of your input and is contained in the *CompletionResponse* object. Read more about completions [here](https://docs.aleph-alpha.com/docs/introduction/prompting_and_completion/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompletionResponse(model_version='2022-04', completions=[CompletionResult(log_probs=None, completion=' Artificial Intelligence is the simulation of human intelligence processes by machines.\\n', completion_tokens=None, finish_reason='stop_sequence_reached: Q:')], optimized_prompt=None)\n",
      "\n",
      "Text Completion: Artificial Intelligence is the simulation of human intelligence processes by machines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the CompletionResponse to the prompt with metadata\n",
    "print(completion_response)\n",
    "\n",
    "# Display only the text completion without the metadata\n",
    "print(\"\\nText Completion:\" + completion_response.completions[0].completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available parameters and models\n",
    "The *CompletionRequest* has many parameters that can be set to change the behaviour of the model.\n",
    "\n",
    "You can read more about the parameters in our [API documentation](https://docs.aleph-alpha.com/api/) or try them out in our [Playground](https://app.aleph-alpha.com/playground/completion).\n",
    "\n",
    "Feel free to use our other currently available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'luminous-extended', 'description': 'Multilingual model trained on English, German, French, Spanish and Italian', 'max_context_size': 2048, 'hostings': ['aleph-alpha'], 'image_support': True, 'qa_support': True, 'summarization_support': True, 'embedding_types': []}, {'name': 'luminous-base', 'description': 'Multilingual model trained on English, German, French, Spanish and Italian', 'max_context_size': 2048, 'hostings': ['aleph-alpha'], 'image_support': True, 'qa_support': False, 'summarization_support': False, 'embedding_types': ['symmetric', 'symmetric_128', 'asymmetric_document', 'asymmetric_query', 'asymmetric_128_document', 'asymmetric_128_query']}, {'name': 'luminous-supreme', 'description': 'Multilingual model trained on English, German, French, Spanish and Italian', 'max_context_size': 2048, 'hostings': ['aleph-alpha'], 'image_support': False, 'qa_support': False, 'summarization_support': False, 'embedding_types': []}]\n"
     ]
    }
   ],
   "source": [
    "client = AlephAlphaClient(token=\"API_TOKEN\")\n",
    "print(client.available_models())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('aa-base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fdd045fef95f4447f1c9ba907f4ef4fc82393cda2753ca8fbebe6283025a7d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
